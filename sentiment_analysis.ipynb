{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "d0zuPb1yPX3_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JWbsgxyD66_K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ArJSQNqBOWWm"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# os.environ['KAGGLE_USERNAME'] = \"\"\n",
        "# os.environ['KAGGLE_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_4i_0tkOexx",
        "outputId": "a971cc7b-efb3-4bb4-aea3-d883c46e254c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "twitter-entity-sentiment-analysis.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOp__-BtOk6_",
        "outputId": "3b8627df-3dea-4714-89ab-fab8aa48740c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/twitter-entity-sentiment-analysis.zip\n",
            "replace /content/twitter_training.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /content/twitter-entity-sentiment-analysis.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuYd44jg5Iyg",
        "outputId": "4d795eb4-4a8b-4430-ff24-7634e1168e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at content/; to attempt to forcibly remount, call drive.mount(\"content/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('content/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB5DcDsuPEkY"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6jGgA0IrWGz",
        "outputId": "5abcc6ce-b9be-4235-aeb9-cb17b9fa2991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gtcL_gz8rLc3"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xskSsBQPPGnS"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = '/content/twitter_training.csv'\n",
        "TEST_PATH = '/content/twitter_validation.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "xgs_1HWuPSPY"
      },
      "outputs": [],
      "source": [
        "col_names = ['sn', 'org', 'sentiment', 'text']\n",
        "train_df = pd.read_csv(TRAIN_PATH, header=None, names=col_names)\n",
        "test_df = pd.read_csv(TEST_PATH, header=None, names=col_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lHq49Dm0PzHd",
        "outputId": "cae46fa2-ded3-4e80-89ea-1cd2096fbcfc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-38dcbf9d-dd1e-4178-9f58-2a33b8e46c2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sn</th>\n",
              "      <th>org</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>74677</th>\n",
              "      <td>9200</td>\n",
              "      <td>Nvidia</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Just realized that the Windows partition of my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74678</th>\n",
              "      <td>9200</td>\n",
              "      <td>Nvidia</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Just realized that my Mac window partition is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74679</th>\n",
              "      <td>9200</td>\n",
              "      <td>Nvidia</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Just realized the windows partition of my Mac ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74680</th>\n",
              "      <td>9200</td>\n",
              "      <td>Nvidia</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Just realized between the windows partition of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74681</th>\n",
              "      <td>9200</td>\n",
              "      <td>Nvidia</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Just like the windows partition of my Mac is l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38dcbf9d-dd1e-4178-9f58-2a33b8e46c2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38dcbf9d-dd1e-4178-9f58-2a33b8e46c2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38dcbf9d-dd1e-4178-9f58-2a33b8e46c2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         sn     org sentiment  \\\n",
              "74677  9200  Nvidia  Positive   \n",
              "74678  9200  Nvidia  Positive   \n",
              "74679  9200  Nvidia  Positive   \n",
              "74680  9200  Nvidia  Positive   \n",
              "74681  9200  Nvidia  Positive   \n",
              "\n",
              "                                                    text  \n",
              "74677  Just realized that the Windows partition of my...  \n",
              "74678  Just realized that my Mac window partition is ...  \n",
              "74679  Just realized the windows partition of my Mac ...  \n",
              "74680  Just realized between the windows partition of...  \n",
              "74681  Just like the windows partition of my Mac is l...  "
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "a2wIetT6gTm2",
        "outputId": "aa8cbffc-2f96-4151-c6a6-25fde5887e3f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a7576c0d-b45f-49f9-bc16-5ffcf16923d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sn</th>\n",
              "      <th>org</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3364</td>\n",
              "      <td>Facebook</td>\n",
              "      <td>Irrelevant</td>\n",
              "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>352</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8312</td>\n",
              "      <td>Microsoft</td>\n",
              "      <td>Negative</td>\n",
              "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4371</td>\n",
              "      <td>CS-GO</td>\n",
              "      <td>Negative</td>\n",
              "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4433</td>\n",
              "      <td>Google</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Now the President is slapping Americans in the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7576c0d-b45f-49f9-bc16-5ffcf16923d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7576c0d-b45f-49f9-bc16-5ffcf16923d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7576c0d-b45f-49f9-bc16-5ffcf16923d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     sn        org   sentiment  \\\n",
              "0  3364   Facebook  Irrelevant   \n",
              "1   352     Amazon     Neutral   \n",
              "2  8312  Microsoft    Negative   \n",
              "3  4371      CS-GO    Negative   \n",
              "4  4433     Google     Neutral   \n",
              "\n",
              "                                                text  \n",
              "0  I mentioned on Facebook that I was struggling ...  \n",
              "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
              "2  @Microsoft Why do I pay for WORD when it funct...  \n",
              "3  CSGO matchmaking is so full of closet hacking,...  \n",
              "4  Now the President is slapping Americans in the...  "
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuTjZhF_Tq4I",
        "outputId": "ba38173f-b4ad-468f-84a4-353543253f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(74682, 4)\n",
            "(1000, 4)\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bwNSe3B5nqFZ"
      },
      "outputs": [],
      "source": [
        "# train_df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM4dmHIhoMJR"
      },
      "source": [
        "## label counts\n",
        "-   -1    22542\n",
        "-   1    20832\n",
        "-   0    18318"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "GhC_oh9qnTYm"
      },
      "outputs": [],
      "source": [
        "label_mapping = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\n",
        "\n",
        "def rm_cols(df):\n",
        "  df = df.dropna(subset=['text'])\n",
        "  df = df.drop(['sn', 'org'], axis=1)\n",
        "  df = df[~df['sentiment'].isin(['Irrelevant'])]\n",
        "  df['sentiment']= df['sentiment'].map(label_mapping)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "75PbFBPCuqE3"
      },
      "outputs": [],
      "source": [
        "train_df = rm_cols(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWiUy9zguwjj",
        "outputId": "4fe21507-9fa7-4377-9fa6-812325375efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 61121 entries, 0 to 74681\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   sentiment  61121 non-null  int64 \n",
            " 1   text       61121 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "OvTqsQQauweB"
      },
      "outputs": [],
      "source": [
        "test_df = rm_cols(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1CK_b4nvEbp",
        "outputId": "f8a190f9-c752-4723-a845-433cff055811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 828 entries, 1 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   sentiment  828 non-null    int64 \n",
            " 1   text       828 non-null    object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 19.4+ KB\n"
          ]
        }
      ],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "HHo46Hp4snQz"
      },
      "outputs": [],
      "source": [
        "# import transformers\n",
        "\n",
        "# seq_len = 256\n",
        "# batch_size = 16\n",
        "# num_samples = len(train_df)\n",
        "# model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "\n",
        "# tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, padding=True, return_tensor='np')\n",
        "\n",
        "# tokens_train = tokenizer(\n",
        "#     train_df['text'].tolist(),\n",
        "#     max_length=seq_len,\n",
        "#     truncation=True,\n",
        "#     padding='max_length',\n",
        "#     add_special_tokens=True,\n",
        "#     return_tensors='np'\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "M8Gj2lqj3DNT"
      },
      "outputs": [],
      "source": [
        "# num_samples_test = len(test_df)\n",
        "# model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "\n",
        "# test_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, padding=True, return_tensor='np')\n",
        "\n",
        "# tokens_test = tokenizer(\n",
        "#     test_df['text'].tolist(),\n",
        "#     max_length=seq_len,\n",
        "#     truncation=True,\n",
        "#     padding='max_length',\n",
        "#     add_special_tokens=True,\n",
        "#     return_tensors='np'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezg6y2oHFYxg",
        "outputId": "910d5023-255a-44fd-9d1f-5113509710b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Download the pre-trained GloVe embeddings\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "\n",
        "# get the golve file from here\n",
        "# https://drive.google.com/file/d/1Y61lrN9_6WQ_8AkPAq3IHs3I4ITMxgMh/view?usp=sharing\n",
        "glove_file = '/content/content/MyDrive/nlp/glove/glove.6B.100d.txt'\n",
        "\n",
        "def load_glove_embeddings(glove_file):\n",
        "    word_vectors = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            word_vectors[word] = vector\n",
        "    return word_vectors\n",
        "\n",
        "word_vectors = load_glove_embeddings(glove_file)\n",
        "# Preprocess and tokenize the text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTY3I-nMO6Eo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "YSET3DHWN3Qx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame train_df with 'text' column\n",
        "\n",
        "def convert_to_word_embeddings(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in word_vectors:\n",
        "            embeddings.append(word_vectors[token])\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# train_df['word_embeddings'] = train_df['text'].apply(convert_to_word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkCunzbLCWEL",
        "outputId": "9475c231-5581-4361-a8c7-ef1f4c2b1f5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convert_to_word_embeddings([\"hello world\", \"I am the king\", \"[]\"]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If5_Mu9NAokg",
        "outputId": "5b0e3ec2-9861-479b-e440-c758996d1271"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train = train_df.sentiment.values\n",
        "labels_train = np.zeros((y_train.size, 3))\n",
        "labels_train[np.arange(y_train.size), y_train] = 1\n",
        "labels_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5IMi4VSvhL7",
        "outputId": "0482a0cf-166e-437f-9411-c436b98517b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 2, 2, 2])"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ZQZZn1c34j7d"
      },
      "outputs": [],
      "source": [
        "# labels_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBUQMz3KS4Hm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE773lXj6GL-",
        "outputId": "3237f319-765e-4df6-d997-1d69bc716b52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test = test_df.sentiment.values\n",
        "labels_test = np.zeros((y_test.size, 3))\n",
        "labels_test[np.arange(y_test.size), y_test] = 1\n",
        "labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "ub_QlrLKRBX6"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(convert_to_word_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk3M_WNpNonD"
      },
      "source": [
        "# Using pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "BTSY0-qqMjPh"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# sent_pipeline = pipeline(\"sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "5LwUxqsJM3yg"
      },
      "outputs": [],
      "source": [
        "# sent_pipeline('I love you')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8zEsLgmVt1B",
        "outputId": "1241166e-371a-44a4-eba2-79ccc7c45fb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50183"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4urnXfgXQHB"
      },
      "source": [
        "# Building a transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "D1iaOnV5sZVL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "VOCAB_SIZE = 30000\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        self.encoder = Encoder(num_layers, units, d_model, num_heads, dropout)\n",
        "        self.dense = tf.keras.layers.Dense(units)\n",
        "        self.final_dense = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        x = self.encoder(inputs, training=training, mask=mask)\n",
        "        x = tf.reduce_mean(x, axis=1)  # Average pooling over the time dimension\n",
        "        x = self.dense(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.final_dense(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.num_layers = num_layers\n",
        "        self.units = units\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = PositionalEmbedding(VOCAB_SIZE, d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(units, d_model, num_heads, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training=True, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "        super(EncoderLayer, self).__init__(name=name)\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads, d_model)\n",
        "        self.ffn = self.point_wise_feed_forward_network(units, d_model)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training=True, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "    def point_wise_feed_forward_network(self, units, d_model):\n",
        "        return tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Dense(units, activation=\"relu\"),\n",
        "                tf.keras.layers.Dense(d_model),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_encoding = self.positional_encoding()\n",
        "\n",
        "    def positional_encoding(self):\n",
        "        depth = self.d_model // 2\n",
        "        length = 2048\n",
        "        positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
        "        depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
        "\n",
        "        angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
        "        angle_rads = positions * angle_rates  # (pos, depth)\n",
        "\n",
        "        pos_encoding = np.concatenate(\n",
        "            [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "            axis=-1)\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Hw4F5KViAvJ0",
        "outputId": "a9637bf4-4879-4cea-e6c7-b415d761d2b0"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-a28ec0682dd3>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Fit the model using the padded embeddings and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
          ]
        }
      ],
      "source": [
        "save_path = '/content/content/MyDrive'\n",
        "\n",
        "num_layers = 6\n",
        "units = 64\n",
        "d_model = 100\n",
        "num_heads = 5\n",
        "dropout = 0.2\n",
        "num_epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "model = Transformer(num_layers, units, d_model, num_heads, dropout)\n",
        "input_seq = tf.keras.Input(shape=(None,))\n",
        "# mask = tf.keras.Input(shape=(None, None))\n",
        "# output = model(inputs=input_seq, mask=mask)\n",
        "output = model(inputs=input_seq)\n",
        "transformer_model = tf.keras.Model(inputs=input_seq, outputs=output)\n",
        "# transformer_model = tf.keras.Model(inputs=input_seq, outputs=output)\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = f\"{save_path}/sentiment_checkpoint.h5\",\n",
        "    save_best_only = True,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min',\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                          loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "# transformer_model.fit(\n",
        "#     [tokens_train['input_ids'], tokens_train['attention_mask']],\n",
        "#     labels_train,\n",
        "#     epochs=10,\n",
        "#     batch_size=128,\n",
        "#     validation_data=([tokens_test['input_ids'], tokens_test['attention_mask']], labels_test),\n",
        "#     callbacks = [checkpoint_callback],\n",
        "#     verbose = 1\n",
        "#     )\n",
        "\n",
        "\n",
        "# transformer_model.fit(\n",
        "#     tokens_train['input_ids'],\n",
        "#     labels_train,\n",
        "#     epochs=10,\n",
        "#     batch_size=128,\n",
        "#     validation_data=(tokens_test['input_ids'], labels_test),\n",
        "#     callbacks = [checkpoint_callback],\n",
        "#     verbose = 1\n",
        "#     )\n",
        "\n",
        "# model.save(f\"{save_path}/transformer_sentiment_analysis.h5\")\n",
        "# Convert the text column to a list of strings\n",
        "\n",
        "# Convert the word embeddings to a NumPy array\n",
        "# embeddings = np.array(train_df['word_embeddings'].tolist())\n",
        "\n",
        "# Determine the maximum sequence length\n",
        "# max_seq_length = max(len(seq) for seq in embeddings)\n",
        "\n",
        "# # Pad the sequences to the maximum length\n",
        "# padded_embeddings = tf.keras.preprocessing.sequence.pad_sequences(embeddings, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Fit the model using the padded embeddings and labels\n",
        "transformer_model.fit(tf.convert_to_tensor(train_df['text']), tf.convert_to_tensor(labels_train), epochs=num_epochs, batch_size=batch_size)\n",
        "\n",
        "\n",
        "model.save(f\"{save_path}/transformer_sentiment_analysis.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4axynxvk-rMf"
      },
      "source": [
        "## Preditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unKtvV5QPhT4"
      },
      "outputs": [],
      "source": [
        "# custom_objects = {'Transformer': Transformer}\n",
        "\n",
        "# model = tf.keras.models.load_model(\"/content/content/MyDrive/sentiment_checkpoint.h5\",\n",
        "#                                    custom_objects=custom_objects)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0tTSDZi-wed"
      },
      "outputs": [],
      "source": [
        "# import transformers\n",
        "\n",
        "# def prepare_input_data(text):\n",
        "\n",
        "#   seq_len = 256\n",
        "#   model_name = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "\n",
        "#   tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, padding=True, return_tensor='np')\n",
        "\n",
        "#   tokens = tokenizer(\n",
        "#       text,\n",
        "#       max_length=seq_len,\n",
        "#       truncation=True,\n",
        "#       padding='max_length',\n",
        "#       add_special_tokens=True,\n",
        "#       return_tensors='np'\n",
        "#   )\n",
        "#   return [tokens['input_ids'], tokens['attention_mask']]\n",
        "#   #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j0UISFr90dc"
      },
      "outputs": [],
      "source": [
        "# sentiment_mapping = ['Negaitve', 'Neutral', 'Positive']\n",
        "# text_data = [\"This is a positive review.\", \"I did not like this product.\", \"The movie was okay.\", \"I love you\"]\n",
        "# input_data = prepare_input_data(text_data)\n",
        "\n",
        "# predictions = transformer_model.predict(input_data)\n",
        "\n",
        "# predicted_labels = tf.argmax(predictions, axis=1)\n",
        "\n",
        "# for i, text in enumerate(text_data):\n",
        "#     print(predicted_labels)\n",
        "#     print(f\"Text: {text}\")\n",
        "#     print(f\"Predicted Label: {predicted_labels[i]}\")\n",
        "#     print(f\"Sentiment: {sentiment_mapping[predicted_labels[i]]}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY61sBu7TTOS"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# y_pred = model.predict([tokens_test['input_ids'], tokens_test['attention_mask']])\n",
        "# y_pred = tf.argmax(y_pred)\n",
        "# print(accuracy_score(y_pred, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
